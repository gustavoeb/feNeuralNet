//
// Developed by Gustavo E. Boehs
//
/**
The Net_d object represents a neural network. It supports multiple outputs and hidden layers.
All layers are assumed to be fully connected, so it is not posible to represent some
topologies like TDNNs and Convolutional Neural Nets.

Known issues:
* softmax layer overflow
* no processing of input/output ranges

\rst
  .. kl-example:: Net_d

    // create a fitnet with 3 input features.
    // 1 output feature, and 10 hidden layers.
    Net_d net();
    Layer input('input',3);
    Layer hidden('hidden',10,'tansig');
    Layer output('output',1);
    net.addInputLayer(input);
    net.addConnectedLayer(hidden);
    net.setOutput(net.addConnectedLayer(output));

\endrst
*/

object Net_d {
  Layer[] layers;
  Integer[] inputs;
  Integer[] outputs;
  Mat_d layerWeights[Vec2];
  Mat_d biasWeights[Vec2];
};

/// Constructs empty network
/// \dfgPresetTitle Empty_Net_d
/// \dfgPresetFolder Net/Net_d
function Net_d() {
}

/// \dfgPresetOmit
function Net_d.addLayer!(in Layer layer, out Integer id) {
  // get current layer id
  id = this.layers.size();
  Layer cLayer = layer.clone();
  // add layer
  this.layers.push(cLayer.setIndex(id));
}

/// Sets the weights between connections
/// \param connection A Vec2 where x=child, y=parent.
/// \param weights The connection's weights
/// \dfgPresetOmit
function Net_d.setWeights!(Vec2 connection, Mat_d weights) {
  Integer child = connection.x;
  Integer parent = connection.y;

  // check if layers exist
  if(child >= this.layers.size())
    throw FUNC + ": layer id (" + child + ") does not exist in this network.";

  // check dimentions
  Integer rows = this.layers[child].getNeuronSize();
  Integer cols = this.layers[parent].getNeuronSize();
  if(weights.cols() != cols || weights.rows() != rows)
    throw FUNC + "weight matrix does not match layers' dimentions (" + rows + ", " + cols + " vs " + weights.rows() + ", " + weights.cols() + ")";

  // and add connections
  this.layerWeights[connection] = weights;
}

/// Sets the bias weights between connections
/// \param connection A Vec2 where x=child, y=parent.
/// \param weights The bias' weights
/// \dfgPresetOmit
function Net_d.setBias!(Vec2 connection, Mat_d bias) {
  Integer child = connection.x;
  Integer parent = connection.y;

  // check if layers exist
  if(child >= this.layers.size())
    throw FUNC + ": layer id (" + child + ") does not exist in this network.";

  // check dimentions
  Integer rows = this.layers[child].getNeuronSize();
  if(bias.cols() != 1 || bias.rows() != rows)
    throw FUNC + "bias weight matrix does not match layers' dimentions (" + rows + ", 1 vs " + bias.rows() + ", " + bias.cols() + ")";

  // and add connections
  this.biasWeights[connection] = bias;
}

/// \dfgPresetOmit
function Net_d.addInputLayer!(in Layer layer, out Integer id) {
  Layer cLayer = layer.clone();
  // add layer
  this.addLayer(cLayer.setParentIndex(-1), id); // inputs have -1 as parents
  // aditionaly we store their indices;
  this.inputs.push(id);
}

/// \dfgPresetOmit
function Net_d.addConnectedLayer!(in Layer layer, out Integer id) {
  Integer child = this.layers.size();
  Integer parent = child-1;
  Layer cLayer = layer.clone();

  // check for previous layers
  if(child == 0)
    throw FUNC + "can't connect layers in an empty network. Add an input layer first.";
    
  // add layer
  this.addLayer(cLayer.setParentIndex(parent), id);
}

/// \dfgPresetTitle AddConnecterLayer_Weights
/// \dfgPresetOmit
function Net_d.addConnectedLayer!(in Layer layer, in Mat_d weights, out Integer id) {
  Integer child = this.layers.size();
  Integer parent = child-1;

  this.addConnectedLayer(layer, id);

  // add weights
  this.setWeights(Vec2(Scalar(child),Scalar(parent)), weights);
}

/// \dfgPresetTitle AddConnecterLayer_WeightsAndBias
/// \dfgPresetOmit
function Net_d.addConnectedLayer!(in Layer layer, in Mat_d weights, in Mat_d bias, out Integer id) {
  Integer child = this.layers.size();
  Integer parent = child-1;

  this.addConnectedLayer(layer, weights, id);

  // add weights
  this.setBias(Vec2(Scalar(id),Scalar(parent)), bias);
}

/// \dfgPresetOmit
function Net_d.setOutput!(Integer id) {
  // check if layers exists
  if(id >= this.layers.size())
    throw FUNC + ": layer id (" + id + ") does not exist in this network.";

  this.outputs.push(id);
}

/// \dfgPresetOmit
function Net_d.setLastAsOutput!() {
  // check if layers exists
  if(this.layers.size() <= 0)
    throw FUNC + ": network has no layers.";

  this.outputs.push(this.layers.size()-1);
}

/// A recursive function used in the feedForward step.
/// \param id The id of the evaluated layer.
/// \param input The features being inputed for the feedForward step.
/// \dfgPresetOmit
function Mat_d Net_d.evalLayer(Integer id, Mat_d input) {
  Mat_d features;
  Mat_d output;
  Mat_d layWei;
  Mat_d biasWei;
  Integer parent = this.layers[id].getParentIndex();
  Boolean hasBias = false; // We assume layers have no bias.

  // If layer is not connected to -1 (inputs), we
  // need to dig recursevly.
  if(parent != -1) {
    // We pass the inputs until finding the layer.
    // Eventually we should get back the values for this layer
    // and we store it in the features values.
    features = this.evalLayer(parent, input);

    // We should also get the weights betwen this connection.
    if(this.layerWeights.has(Vec2(id,parent)))
      layWei = this.layerWeights[Vec2(id,parent)];

    // And the bias, assuming there are any.
    if(this.biasWeights.has(Vec2(id,parent))) {
      biasWei = this.biasWeights[Vec2(id,parent)];
      hasBias = true; // raise this flag for concatenations
    }
  }
  // If connected to -1, this should be the input layer
  else {
    // if that is the case, return input values from here
    return input;
  }

  // If we have found bias for this layer, we need
  // to concatenate them before multipling weights.
  if(hasBias) {
    Mat_d featBias(1, features.cols()); featBias.setAll(1.0);
    features.vertcat(featBias);
    layWei.horzcat(biasWei);
  }

  // Finnally we multiply weights
  output = layWei * features;

  // We squash values based on the layers defined
  // non-linearity.
  String transf = this.layers[id].getTransferFunc();
  if(transf == 'tansig')
    output.setAll(tansig(output.getAll()));
  else if(transf == 'logsig')
    output.setAll(logsig(output.getAll()));
  else if(transf == 'softmax')
    output = softmax(output);

  // And return values for next recursive iteration.
  return output;
}

/// A feedForward step.
/// \param input The features being inputed for the feedForward step.
/// \dfgPresetTitle FeedForward
/// \dfgPresetFolder Sim
/// \dfgPresetType input $TYPE$
/// \dfgPresetOmit
function Mat_d Net_d.feedForward(Mat_d input) {
  Integer id = this.outputs[0];
  return this.evalLayer(id, input);
}

/// A feedForward step.
/// \param input The features being inputed for the feedForward step.
/// \dfgPresetOmit
function Float64[] Net_d.feedForward(Float64 input[]) {
  // for now we only support one output
  // this should be substituted by a for loop
  Integer id = this.outputs[0];

  // we construct a single column matrix from our input vector
  Integer elements = input.size();
  Mat_d mat(elements,1);
  mat.setAll(input);

  // and return a vector
  return this.evalLayer(id, mat).getAll();
}

/// \dfgPresetOmit
function Scalar[] Net_d.feedForward(Scalar input[]) {
  // for now we only support one output
  // this should be substituted by a for loop
  Integer id = this.outputs[0];

  // we construct a single column matrix from our input vector
  Integer elements = input.size();
  Mat mat(elements,1);
  mat.setAll(input);

  // and return a vector
  Scalar floatVals[];
  Float64 doubleVals[];
  doubleVals = this.evalLayer(id, Mat_d(mat)).getAll();
  for(Integer i=0; i<doubleVals.size(); i++){
    floatVals.push(Scalar(doubleVals[i]));
  }
  return floatVals;
}